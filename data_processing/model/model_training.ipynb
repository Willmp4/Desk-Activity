{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_data(directory):\n",
    "    session_data = []\n",
    "    for user_folder in os.listdir(directory):\n",
    "        print(\"Loading data for user\", user_folder)\n",
    "        user_path = os.path.join(directory, user_folder)\n",
    "        if os.path.isdir(user_path):\n",
    "            for date_folder in os.listdir(user_path):\n",
    "                print(\"Loading data for date\", date_folder)\n",
    "                date_path = os.path.join(user_path, date_folder)\n",
    "                activity_file = 'activity_log.json'\n",
    "                print(\"Loading data from\", date_path)\n",
    "                file_path = os.path.join(date_path, activity_file)\n",
    "                if os.path.isfile(file_path):\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        file_data = json.load(file)\n",
    "                        if isinstance(file_data, list):\n",
    "                            session_data.append(file_data)  # Each file is one session\n",
    "    return session_data\n",
    "\n",
    "def preprocess_data(session_data, threshold=5):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    for session in session_data:\n",
    "        print(\"Processing session with\", len(session), \"events\")\n",
    "        features = []\n",
    "        label = None\n",
    "        for event in session:\n",
    "            # Check if the event is a focus level event and extract \n",
    "            if 'focus_level' in event['type']:\n",
    "                focus_level = event['data']['level']\n",
    "                label = 1 if focus_level > threshold else 0\n",
    "                if features and label is not None:  # Ensure there is data to add before resetting\n",
    "                    print(\"Adding session with\", len(features), \"events\")\n",
    "                    all_features.append(features)\n",
    "                    all_labels.append(label)\n",
    "                # Reset features and label for a new session starting after this event\n",
    "                features = []\n",
    "            # Extract features based on event type\n",
    "            else:\n",
    "                event_type = event['type']\n",
    "                time_delta = event['data'].get('time_delta', 0)\n",
    "                if event_type == 'gaze_data':\n",
    "                    position = event['data'].get('adjusted_gaze_start_position', [0, 0])\n",
    "                elif event_type == 'mouse_movement':\n",
    "                    start_position = event['data'].get('start_position', [0, 0])\n",
    "                    end_position = event['data'].get('end_position', [0, 0])\n",
    "                    position = [(s + e) / 2 for s, e in zip(start_position, end_position)]  # Average position\n",
    "                elif event_type == 'mouse_click':\n",
    "                    position = event['data'].get('position', [0, 0])\n",
    "                elif event_type == 'keyboard_session':\n",
    "                    position = event['data'].get('key_stokes', [0, 0])\n",
    "                else:\n",
    "                    position = [0, 0]\n",
    "\n",
    "                button = event['data'].get('button', 'None')\n",
    "                feature = [event['timestamp'], event_type, position, button, time_delta]\n",
    "                features.append(feature)\n",
    "    return all_features, all_labels\n",
    "\n",
    "\n",
    "\n",
    "def encode_features(features):\n",
    "    # Collect all categories for fitting the encoder\n",
    "    all_categories = []\n",
    "    all_time_deltas = []\n",
    "    for session in features:\n",
    "        all_categories.extend([[feat[1], feat[3]] for feat in session])\n",
    "        all_time_deltas.extend([feat[4] for feat in session])  # Extract time deltas\n",
    "\n",
    "    encoder = OneHotEncoder()\n",
    "    encoder.fit(all_categories)  # Fit encoder to all categories once\n",
    "\n",
    "    # Normalize time deltas\n",
    "    scaler = StandardScaler()\n",
    "    all_time_deltas = np.array(all_time_deltas).reshape(-1, 1)  # Reshape for scaling\n",
    "    scaler.fit(all_time_deltas)\n",
    "    all_time_deltas_normalized = scaler.transform(all_time_deltas).flatten()\n",
    "\n",
    "    all_sessions = []\n",
    "    time_delta_index = 0\n",
    "    for session in features:\n",
    "        categorical_features = np.array([[feat[1], feat[3]] for feat in session])\n",
    "        categorical_encoded = encoder.transform(categorical_features).toarray()\n",
    "        position_data = np.array([feat[2] for feat in session])\n",
    "        time_deltas = np.array([all_time_deltas_normalized[time_delta_index:time_delta_index+len(session)]])\n",
    "        time_delta_index += len(session)\n",
    "        encoded_session = np.hstack((position_data, categorical_encoded, time_deltas.T))  # Append time deltas\n",
    "        all_sessions.append(encoded_session)\n",
    "\n",
    "    return all_sessions, encoder, scaler\n",
    "\n",
    "\n",
    "\n",
    "def create_sequences(features, labels, sequence_length=100):\n",
    "\n",
    "    # Padding sequences\n",
    "    padded_features = pad_sequences(features, maxlen=sequence_length, padding='post', dtype='float32')\n",
    "    padded_labels = np.array(labels)  # No need to pad labels as there is one per sequence\n",
    "    print(padded_features.shape, padded_labels.shape)\n",
    "    return padded_features, padded_labels\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "def build_and_train_model(sequences, labels):\n",
    "    # Convert list to numpy array if not already\n",
    "    sequences = np.array(sequences)\n",
    "    \n",
    "    # Check if sequences array is not empty\n",
    "    if sequences.size > 0:\n",
    "        print(sequences.shape)\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Add bidirectional LSTMs and more layers\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, input_shape=(sequences.shape[1], sequences.shape[2]))))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Bidirectional(LSTM(128)))\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "        # More dense layers and complex network topology\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        # Compile the model with an optimizer and loss function\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        # Convert labels to numpy array if not already\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(sequences, labels, epochs=50, batch_size=64)\n",
    "        return model\n",
    "    else:\n",
    "        print(\"No valid sequences to train on.\")\n",
    "        return None\n",
    "\n",
    "import pickle\n",
    "def save_model_and_preprocessors(model, encoder, scaler, model_path, encoder_path, scaler_path):\n",
    "    # Save the Keras model\n",
    "    model.save(model_path)\n",
    "    # Save the preprocessors\n",
    "    with open(encoder_path, 'wb') as f:\n",
    "        pickle.dump(encoder, f)\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "def main():\n",
    "    directory = '../../focus_level/'\n",
    "    session_data = load_data(directory)\n",
    "    processed_data, labels = preprocess_data(session_data)\n",
    "    encoded_features, encoder, scaler = encode_features(processed_data)\n",
    "    X, y = create_sequences(encoded_features, labels)\n",
    "\n",
    "    model = build_and_train_model(X, y)\n",
    "\n",
    "    # Save model and preprocessors\n",
    "    model_path = 'model.h5'\n",
    "    encoder_path = 'encoder.pkl'\n",
    "    scaler_path = 'scaler.pkl'\n",
    "    save_model_and_preprocessors(model, encoder, scaler, model_path, encoder_path, scaler_path)\n",
    "\n",
    "    return model, encoder, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_model_and_preprocessors(model, encoder, scaler, model_path, encoder_path, scaler_path):\n",
    "    model.save(model_path)\n",
    "    with open(encoder_path, 'wb') as f:\n",
    "        pickle.dump(encoder, f)\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for user User\n",
      "Loading data for date 2024-05-11\n",
      "Loading data from ../../focus_level/User\\2024-05-11\n",
      "Loading data for user wgoud\n",
      "Loading data for date 2024-04-12\n",
      "Loading data from ../../focus_level/wgoud\\2024-04-12\n",
      "Loading data for date 2024-04-14\n",
      "Loading data from ../../focus_level/wgoud\\2024-04-14\n",
      "Loading data for date 2024-04-16\n",
      "Loading data from ../../focus_level/wgoud\\2024-04-16\n",
      "Loading data for date 2024-04-21\n",
      "Loading data from ../../focus_level/wgoud\\2024-04-21\n",
      "Loading data for date 2024-04-23\n",
      "Loading data from ../../focus_level/wgoud\\2024-04-23\n",
      "Loading data for date 2024-04-25\n",
      "Loading data from ../../focus_level/wgoud\\2024-04-25\n",
      "Loading data for date 2024-04-29\n",
      "Loading data from ../../focus_level/wgoud\\2024-04-29\n",
      "Loading data for date 2024-04-30\n",
      "Loading data from ../../focus_level/wgoud\\2024-04-30\n",
      "Processing session with 30027 events\n",
      "Adding session with 11603 events\n",
      "Adding session with 6213 events\n",
      "Adding session with 652 events\n",
      "Adding session with 6940 events\n",
      "Adding session with 2720 events\n",
      "Adding session with 347 events\n",
      "Adding session with 1545 events\n",
      "Processing session with 1497 events\n",
      "Adding session with 1496 events\n",
      "Processing session with 5848 events\n",
      "Adding session with 5846 events\n",
      "Processing session with 4257 events\n",
      "Adding session with 4256 events\n",
      "Processing session with 218 events\n",
      "Adding session with 217 events\n",
      "Processing session with 17977 events\n",
      "Adding session with 6886 events\n",
      "Adding session with 2813 events\n",
      "Adding session with 901 events\n",
      "Adding session with 4936 events\n",
      "Adding session with 2436 events\n",
      "Processing session with 16395 events\n",
      "Adding session with 8169 events\n",
      "Adding session with 8224 events\n",
      "Processing session with 21595 events\n",
      "Adding session with 11880 events\n",
      "Adding session with 9712 events\n",
      "Processing session with 21082 events\n",
      "Adding session with 10870 events\n",
      "Adding session with 10210 events\n",
      "(22, 100, 14) (22,)\n",
      "(22, 100, 14)\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.7545 - accuracy: 0.5455\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.5426 - accuracy: 0.7273\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4902 - accuracy: 0.8182\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3938 - accuracy: 0.8182\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3890 - accuracy: 0.7727\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4058 - accuracy: 0.8182\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3729 - accuracy: 0.7727\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2656 - accuracy: 0.9545\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2223 - accuracy: 0.9545\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.2089 - accuracy: 0.9091\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2432 - accuracy: 0.8636\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1862 - accuracy: 0.9091\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1620 - accuracy: 0.9545\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.1376 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1160 - accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0996 - accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0759 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1025 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0544 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0394 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0423 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0537 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0266 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0247 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0018 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    model = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Prediction: [[0.9831607]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load Model and Preprocessors\n",
    "def load_model_and_preprocessors(model_path, encoder_path, scaler_path):\n",
    "    model = load_model(model_path)\n",
    "    with open(encoder_path, 'rb') as f:\n",
    "        encoder = pickle.load(f)\n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    return model, encoder, scaler\n",
    "\n",
    "# Process a single JSON file\n",
    "def process_json_file(file_path, encoder, scaler, threshold=5):\n",
    "    with open(file_path, 'r') as file:\n",
    "        session_data = json.load(file)\n",
    "\n",
    "    features = []\n",
    "    for event in session_data:\n",
    "        if event['type'] != 'focus_level':\n",
    "            event_type = event['type']\n",
    "            time_delta = event['data'].get('time_delta', 0)\n",
    "            if event_type == 'gaze_data':\n",
    "                position = event['data'].get('adjusted_gaze_start_position', [0, 0])\n",
    "            elif event_type == 'mouse_movement':\n",
    "                start_position = event['data'].get('start_position', [0, 0])\n",
    "                end_position = event['data'].get('end_position', [0, 0])\n",
    "                position = [(s + e) / 2 for s, e in zip(start_position, end_position)]\n",
    "            elif event_type == 'mouse_click':\n",
    "                position = event['data'].get('position', [0, 0])\n",
    "            elif event_type == 'keyboard_session':\n",
    "                position = event['data'].get('key_stokes', [0, 0])\n",
    "            else:\n",
    "                position = [0, 0]\n",
    "\n",
    "            button = event['data'].get('button', 'None')\n",
    "            feature = [event['timestamp'], event_type, position, button, time_delta]\n",
    "            features.append(feature)\n",
    "\n",
    "    # Encode and scale features\n",
    "    if features:\n",
    "        categorical_features = np.array([[feat[1], feat[3]] for feat in features])\n",
    "        position_data = np.array([feat[2] for feat in features])\n",
    "        time_deltas = np.array([feat[4] for feat in features]).reshape(-1, 1)\n",
    "        time_deltas_normalized = scaler.transform(time_deltas).flatten()\n",
    "\n",
    "        categorical_encoded = encoder.transform(categorical_features).toarray()\n",
    "        encoded_session = np.hstack((position_data, categorical_encoded, time_deltas_normalized[:, np.newaxis]))\n",
    "\n",
    "        # Padding sequences\n",
    "        padded_features = pad_sequences([encoded_session], maxlen=100, padding='post', dtype='float32')\n",
    "\n",
    "        return padded_features\n",
    "    return None\n",
    "\n",
    "# Predict function\n",
    "def predict_from_json(file_path, model, encoder, scaler):\n",
    "    processed_features = process_json_file(file_path, encoder, scaler)\n",
    "    if processed_features is not None:\n",
    "        prediction = model.predict(processed_features)\n",
    "        return prediction\n",
    "    return \"No valid data to predict.\"\n",
    "\n",
    "# Main entry to test the model with a specified JSON file\n",
    "if __name__ == '__main__':\n",
    "    model_path = 'model.h5'\n",
    "    encoder_path = 'encoder.pkl'\n",
    "    scaler_path = 'scaler.pkl'\n",
    "    json_file_path = 'events_data.json'\n",
    "\n",
    "    model, encoder, scaler = load_model_and_preprocessors(model_path, encoder_path, scaler_path)\n",
    "    prediction = predict_from_json(json_file_path, model, encoder, scaler)\n",
    "    print(\"Prediction:\", prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
